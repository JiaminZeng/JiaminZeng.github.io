---
layout: post
title: 统计数学-线性代数相关
categories: Math
description: 记录线性代数一些乱七八糟的东西
keywords: keyword1, keyword2
---

## 基本定义

### 对称矩阵

是指元素以主对角线为对称轴对应相等的矩阵，矩阵与其逆相乘必定为对称矩阵。

### 正交矩阵

$$A^TA=AA^T=E$$

$$A=A^T=A^{-1}$$

- 等式两边取行列式，得到A的行列式值是 $\pm1$
- 正交矩阵 $A$ 的行向量组以及列向量组都是标准正交的向量组

### 对角矩阵

除了主对角线之外其它元素全部为零的矩阵

### 半正定矩阵

任意的 $A_{m\times n}$，$A^TA$ 为半正定矩阵

### 对称矩阵对角化

若 $A$ 是实对称矩阵（元素都是实数），则一定存在正交矩阵 $P$，对角矩阵 $\Lambda$，使得下式成立：

$$P^{-1}AP = \Lambda$$

[（证明链接）](https://www.zhihu.com/search?hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2234896186%22%7D&hybrid_search_source=Entity&q=对称矩阵对角化&search_source=Entity&type=content)

## 特征值分解

简单的描述即

$$X\xi = \lambda\xi$$

其中 $X$ 为 $n$ 维方阵，$\xi$ 为列向量，$\lambda$ 为常数。

目前的个人解释： $X$ 相当于一个 $n$ 维坐标系，$\xi$ 向量在坐标系中投影仅会造成**固定比例**的伸缩，而不会产生旋转变换。

## 奇异值分解

半个黑盒...

[知乎](https://zhuanlan.zhihu.com/p/399547902)

> 在我看来，SVD是对数据进行有效特征整理的过程。首先，对于一个m×n矩阵A，我们可以理解为其有m个数据，n个特征，（想象成一个n个特征组成的坐标系中的m个点），然而一般情况下，这n个特征并不是正交的，也就是说这n个特征并不能归纳这个数据集的特征。SVD的作用就相当于是一个坐标系变换的过程，从一个不标准的n维坐标系，转换为一个标准的k维坐标系，并且使这个数据集中的点，到这个新坐标系的欧式距离为最小值（也就是这些点在这个新坐标系中的投影方差最大化），其实就是一个最小二乘的过程。进一步，如何使数据在新坐标系中的投影最大化呢，那么我们就需要让这个新坐标系中的基尽可能的不相关，我们可以用协方差来衡量这种相关性。A^T·A中计算的便是n×n的协方差矩阵，每一个值代表着原来的n个特征之间的相关性。当对这个协方差矩阵进行特征分解之后，我们可以得到奇异值和右奇异矩阵，而这个右奇异矩阵则是一个新的坐标系，奇异值则对应这个新坐标系中每个基对于整体数据的影响大小，我们这时便可以提取奇异值最大的k个基，作为新的坐标，这便是PCA的原理。

## 主成分分析
